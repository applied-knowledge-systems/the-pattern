{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "color-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desirable-syntax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redis Cluster is not available\n"
     ]
    }
   ],
   "source": [
    "tokenizer = None \n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "\n",
    "config_switch=os.getenv('DOCKER', 'local')\n",
    "if config_switch=='local':\n",
    "    startup_nodes = [{\"host\": \"127.0.0.1\", \"port\": \"30001\"}, {\"host\": \"127.0.0.1\", \"port\":\"30002\"}, {\"host\":\"127.0.0.1\", \"port\":\"30003\"}]\n",
    "else:\n",
    "    startup_nodes = [{\"host\": \"rgcluster\", \"port\": \"30001\"}, {\"host\": \"rgcluster\", \"port\":\"30002\"}, {\"host\":\"rgcluster\", \"port\":\"30003\"}]\n",
    "\n",
    "try: \n",
    "    from redisai import ClusterClient\n",
    "    redisai_cluster_client = ClusterClient(startup_nodes=startup_nodes)\n",
    "except:\n",
    "    print(\"Redis Cluster is not available\")\n",
    "\n",
    "def loadTokeniser():\n",
    "    global tokenizer\n",
    "    from transformers import BertTokenizerFast\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def qa_redisai(question, sentence_key,hash_tag):\n",
    "    ### question is encoded\n",
    "    ### use pre-computed context/answer text tensor\n",
    "\n",
    "    global tokenizer\n",
    "\n",
    "    if not tokenizer:\n",
    "        tokenizer=loadTokeniser()\n",
    "\n",
    "     \n",
    "\n",
    "    token_key = f\"tokenized:bert:qa:{sentence_key}\"\n",
    "\n",
    "    input_ids_question = tokenizer.encode(question, add_special_tokens=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "\n",
    "    \n",
    "    input_ids_context=redisai_cluster_client.tensorget(token_key)\n",
    "    input_ids = np.append(input_ids_question,input_ids_context)\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    print(input_ids)\n",
    "    attention_mask = np.array([[1]*len(input_ids)])\n",
    "    input_idss=np.array([input_ids])\n",
    "    print(input_idss.shape)\n",
    "    print(\"Attention mask shape \",attention_mask.shape)\n",
    "    \n",
    "    num_seg_a=input_ids_question.shape[1]\n",
    "    print(num_seg_a)\n",
    "    num_seg_b=input_ids_context.shape[0]\n",
    "    print(num_seg_b)\n",
    "    token_type_ids = np.array([0]*num_seg_a + [1]*num_seg_b)\n",
    "    print(\"Segments id\",token_type_ids.shape)\n",
    "    \n",
    "    redisai_cluster_client.tensorset(f'input_ids{hash_tag}', input_idss)\n",
    "    redisai_cluster_client.tensorset(f'attention_mask{hash_tag}', attention_mask)\n",
    "    redisai_cluster_client.tensorset(f'token_type_ids{hash_tag}', token_type_ids)\n",
    "\n",
    "    redisai_cluster_client.modelrun(f'bert-qa{hash_tag}', [f'input_ids{hash_tag}', f'attention_mask{hash_tag}', f'token_type_ids{hash_tag}'],\n",
    "                        [f'answer_start_scores{hash_tag}', f'answer_end_scores{hash_tag}'])\n",
    "    print(f\"Model run on {hash_tag}\")\n",
    "    answer_start_scores = redisai_cluster_client.tensorget(f'answer_start_scores{hash_tag}')\n",
    "    answer_end_scores = redisai_cluster_client.tensorget(f'answer_end_scores{hash_tag}')\n",
    "\n",
    "    answer_start = np.argmax(answer_start_scores)\n",
    "    answer_end = np.argmax(answer_end_scores) + 1\n",
    "    \n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end], skip_special_tokens = True))\n",
    "    print(answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "occasional-fitness",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182,)\n",
      "[  101  2054  2055 13139  1997  5258  8625  5897 12987  1029   102  1056\n",
      "  1044  1041  1042  1054  1041  1053  1057  1041  1050  1039  1045  1041\n",
      "  1055  1051  1042  1051  1039  1039  1057  1054  1054  1041  1050  1039\n",
      "  1041  1042  1051  1054  1045  1050  1057  1039  1048  1041  1051  1056\n",
      "  1045  1040  1041  1055  1059  1041  1054  1041  1039  1051  1049  1052\n",
      "  1037  1054  1041  1040  1056  1051  1056  1044  1041  1054  1037  1050\n",
      "  1040  1051  1049  1054  1050  1037  1039  1051  1057  1050  1056  1041\n",
      "  1054  1052  1037  1054  1056  1055  1044  1037  1058  1045  1050  1043\n",
      "  1056  1044  1041  1055  1037  1049  1041  1038  1037  1055  1041  1052\n",
      "  1054  1051  1052  1051  1054  1056  1045  1051  1050  1045  1050  1051\n",
      "  1054  1040  1041  1054  1056  1051  1039  1051  1049  1052  1057  1056\n",
      "  1041  1056  1044  1041  1037  1058  1037  1048  1057  1041  1056  1044\n",
      "  1037  1056  1054  1041  1042  1048  1041  1039  1056  1041  1040  1056\n",
      "  1044  1041  1045  1054  1045  1050  1057  1039  1048  1041  1051  1056\n",
      "  1045  1040  1041  1038  1045  1037  1055  1056  1037  1038  1048  1041\n",
      "  1016   102]\n",
      "(1, 182)\n",
      "Attention mask shape  (1, 182)\n",
      "11\n",
      "171\n",
      "Segments id (182,)\n",
      "Model run on {06S}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What about frequencies of occurenence RNA?\"\n",
    "qa_redisai(question,\"PMC222961.xml:{06S}:26\",'{06S}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Effectiveness of community contact reduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_key=\"PMC261870.xml:{06S}:26\"\n",
    "token_key = f\"tokenized:bert:qa:{sentence_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "redisai_cluster_client.connection_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "slot = redisai_cluster_client.connection_pool.nodes.keyslot(sentence_key)\n",
    "node = redisai_cluster_client.connection_pool.get_master_node_by_slot(slot)\n",
    "connection = redisai_cluster_client.connection_pool.get_connection_by_node(node)\n",
    "connection.send_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)\n",
    "print(connection.__dict__)\n",
    "print(redisai_cluster_client.parse_response(connection,\"RG.TRIGGER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "slot = redisai_cluster_client.connection_pool.nodes.keyslot(sentence_key)\n",
    "node = redisai_cluster_client.connection_pool.get_master_node_by_slot(slot)\n",
    "connection = redisai_cluster_client.connection_pool.get_connection_by_node(node)\n",
    "connection.send_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)\n",
    "print(connection.__dict__)\n",
    "print(redisai_cluster_client.parse_response(connection,\"RG.TRIGGER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rediscluster import RedisCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "startup_nodes = [{\"host\": \"127.0.0.1\", \"port\": \"30001\"}, {\"host\": \"127.0.0.1\", \"port\":\"30002\"}, {\"host\":\"127.0.0.1\", \"port\":\"30003\"}]\n",
    "rc = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_methods = [method_name for method_name in dir(rc)\n",
    "                  if callable(getattr(rc, method_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_key=\"PMC261870.xml:{06S}:26\"\n",
    "question=\"Effectiveness of community contact reduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.execute_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "command='RG.TRIGGER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.determine_node('RG.TRIGGER',\"RunQABERT\",sentence_key,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rc.nodes_flags.get(command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "args=[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(args)>=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.execute_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-costa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rediscluster import RedisCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from rediscluster import RedisCluster\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger('rediscluster')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.execute_command('RG.TRIGGER',\"RunQABERT\",sentence_key,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.connection_pool.nodes.random_node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(rc.connection_pool.nodes.all_masters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.get(sentence_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rc.parse_response(connection,\"RG.TRIGGER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=rc.get(\"cache{06S}_PMC261870.xml:{06S}:26_Effectiveness of community contact reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "diagnostic-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None \n",
    "model = None\n",
    "\n",
    "import torch\n",
    "\n",
    "def loadTokeniser():\n",
    "    global tokenizer\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", torchscript=True)\n",
    "    return tokenizer\n",
    "\n",
    "def loadModel():\n",
    "    global model\n",
    "    from transformers import AutoModelForQuestionAnswering\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", torchscript=True)\n",
    "    return model\n",
    "\n",
    "def qa(question, content_text):\n",
    "    global tokenizer, model \n",
    "\n",
    "    if not tokenizer:\n",
    "        tokenizer=loadTokeniser()\n",
    "\n",
    "    if not model:\n",
    "        model=loadModel()\n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, content_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    print(input_ids)\n",
    "\n",
    "    answer_start_scores, answer_end_scores = model(**inputs,return_dict=False)\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "christian-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text=\"The frequencies of occurrence for i nucleotides were compared to the random RNA counterparts having the same base proportion in order to compute the a value that reflected their i nucleotide bias Table 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacterial-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What about frequencies of occurenence RNA?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "restricted-victorian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2054, 2055, 13139, 1997, 5258, 8625, 5897, 12987, 1029, 102, 1996, 13139, 1997, 14404, 2005, 1045, 16371, 14321, 26601, 2015, 2020, 4102, 2000, 1996, 6721, 12987, 14562, 2383, 1996, 2168, 2918, 10817, 1999, 2344, 2000, 24134, 1996, 1037, 3643, 2008, 7686, 2037, 1045, 16371, 14321, 26601, 13827, 2795, 1016, 102]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = None \n",
    "model = None\n",
    "\n",
    "import torch\n",
    "\n",
    "def loadTokeniser():\n",
    "    global tokenizer\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    return tokenizer\n",
    "\n",
    "def loadModel():\n",
    "    global model\n",
    "    from transformers import AutoModelForQuestionAnswering\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    return model\n",
    "\n",
    "if not tokenizer:\n",
    "    tokenizer=loadTokeniser()\n",
    "\n",
    "if not model:\n",
    "    model=loadModel()\n",
    "\n",
    "inputs = tokenizer.encode_plus(question, content_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "print(input_ids)\n",
    "\n",
    "answer_start_scores, answer_end_scores = model(**inputs,return_dict=False)\n",
    "answer_start = torch.argmax(\n",
    "    answer_start_scores\n",
    ")  # Get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mathematical-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compared to the random rna counterparts having the same base proportion\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "waiting-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2054, 2055, 13139, 1997, 5258, 8625, 5897, 12987, 1029, 102, 1996, 13139, 1997, 14404, 2005, 1045, 16371, 14321, 26601, 2015, 2020, 4102, 2000, 1996, 6721, 12987, 14562, 2383, 1996, 2168, 2918, 10817, 1999, 2344, 2000, 24134, 1996, 1037, 3643, 2008, 7686, 2037, 1045, 16371, 14321, 26601, 13827, 2795, 1016, 102]\n",
      "compared to the random rna counterparts having the same base proportion\n",
      "CPU times: user 12.4 s, sys: 1.4 s, total: 13.8 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(qa(question,content_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "certified-retro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cache{06S}_PMC222961.xml:{06S}:26_What about frequencies of occurenence RNA?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cache{06S}_PMC222961.xml:{06S}:26_%s\" % question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "national-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"When air samples collected?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "champion-result",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cache{5M5}_PMC140314.xml:{5M5}:44_When air samples collected?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cache{5M5}_PMC140314.xml:{5M5}:44_%s\" % question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cordless-showcase",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-cee2556dec3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cache{06S}_PMC261870.xml:{06S}:26_%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rc' is not defined"
     ]
    }
   ],
   "source": [
    "rc.get(\"cache{5M5}_PMC261870.xml:{5M5}:26_%s\" % question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(input_ids):\n",
    "    # BERT only needs the token IDs, but for the purpose of inspecting the \n",
    "    # tokenizer's behavior, let's also get the token strings and display them.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # For each token and its id...\n",
    "    for token, id in zip(tokens, input_ids):\n",
    "\n",
    "        # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "        if id == tokenizer.sep_token_id:\n",
    "            print('')\n",
    "\n",
    "        # Print the token string and its ID in two columns.\n",
    "        print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "        if id == tokenizer.sep_token_id:\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "native-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    " \n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids]),return_dict=False) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cooperative-matrix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 51 tokens.\n",
      "\n",
      "Answer: \"compared to the random rna counterparts having the same base proportion\"\n"
     ]
    }
   ],
   "source": [
    "answer_question(question, content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "hget sentence:PMC222961.xml:{06S} 26\n",
    "\"The frequencies of occurrence for i nucleotides were compared to the random RNA counterparts having the same base proportion in order to compute the a value that reflected their i nucleotide bias Table 2\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "synthetic-macedonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 93 tokens.\n",
      "\n",
      "Answer: \"management of close contacts by home confinement or quarantine and public information and education to encourage prompt reporting of symptoms\"\n"
     ]
    }
   ],
   "source": [
    "context=\"Finally there are many other possible scenarios and as stated in the conclusion of the wHO panel held on 15 17 May 2003 in Geneva Participants from the main outbreak sites noted the striking similarity of the pattern of outbreaks in different countries and the consistent effectiveness of specific control measures including early identification and isolation of patients vigorous contact tracing management of close contacts by home confinement or quarantine and public information and education to encourage prompt reporting of symptoms\"\n",
    "question=\"What about community contact reduction?\"\n",
    "answer_question(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-yemen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
